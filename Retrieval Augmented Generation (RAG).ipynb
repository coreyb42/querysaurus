{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34721282-c19f-4286-8593-3a737bf1b1a5",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1daf270b-8b46-4b76-9502-ab48c4259248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b001da28-7910-4c81-8ba2-6601dd2effc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOllama(model=\"llama2:13b\", \n",
    "                        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L12-v2'\n",
    ")\n",
    "store = FAISS.load_local('core_knowledge',\n",
    "                         embeddings=embedder\n",
    "                        )\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    chat_model,\n",
    "    retriever=store.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad8b353-e579-4a88-9a48-82bb80b0972a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLaMA 2 was released on February 23, 2023, according to the information provided in the text."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'When was LLaMA 2 released?',\n",
       " 'result': ' LLaMA 2 was released on February 23, 2023, according to the information provided in the text.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({'query': 'When was LLaMA 2 released?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f68f264-1a3c-46fa-8d5c-a6db18e8cd6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLaMA 2 is a second-generation language model developed by the non-profit organization Meta AI. It is based on the LLaMA foundation model and was derived from it through fine-tuning. LLaMA 2 is trained on a large dataset of text and is designed to perform well on a wide range of natural language processing tasks, such as text generation, question answering, and conversational dialogue.\n",
      "\n",
      "LLaMA 2 includes several advancements over the original LLaMA model, including:\n",
      "\n",
      "1. Improved performance on downstream tasks: LLaMA 2 is fine-tuned on a larger dataset and has been shown to perform better on a wide range of natural language processing tasks compared to the original LLaMA model.\n",
      "2. Increased interpretability: LLaMA 2 includes several features that make it easier to understand how the model works and how it makes predictions, such as attention visualization and feature importance scoring.\n",
      "3. Improved multi-tasking ability: LLaMA 2 is designed to perform well on a wide range of tasks simultaneously, making it a more versatile and useful tool for natural language processing applications.\n",
      "4. Better handling of out-of-vocabulary words: LLaMA 2 includes several improvements to its vocabulary and word-handling mechanisms, which make it better able to handle out-of-vocabulary words and misspellings compared to the original LLaMA model.\n",
      "5. Increased robustness and generalization ability: LLaMA 2 includes several regularization techniques and other improvements that help it generalize better to new, unseen data and avoid overfitting to training examples.\n",
      "\n",
      "Overall, LLaMA 2 is a more advanced and capable language model compared to the original LLaMA model, and has been shown to perform well on a wide range of natural language processing tasks."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is LLaMA 2?',\n",
       " 'result': ' LLaMA 2 is a second-generation language model developed by the non-profit organization Meta AI. It is based on the LLaMA foundation model and was derived from it through fine-tuning. LLaMA 2 is trained on a large dataset of text and is designed to perform well on a wide range of natural language processing tasks, such as text generation, question answering, and conversational dialogue.\\n\\nLLaMA 2 includes several advancements over the original LLaMA model, including:\\n\\n1. Improved performance on downstream tasks: LLaMA 2 is fine-tuned on a larger dataset and has been shown to perform better on a wide range of natural language processing tasks compared to the original LLaMA model.\\n2. Increased interpretability: LLaMA 2 includes several features that make it easier to understand how the model works and how it makes predictions, such as attention visualization and feature importance scoring.\\n3. Improved multi-tasking ability: LLaMA 2 is designed to perform well on a wide range of tasks simultaneously, making it a more versatile and useful tool for natural language processing applications.\\n4. Better handling of out-of-vocabulary words: LLaMA 2 includes several improvements to its vocabulary and word-handling mechanisms, which make it better able to handle out-of-vocabulary words and misspellings compared to the original LLaMA model.\\n5. Increased robustness and generalization ability: LLaMA 2 includes several regularization techniques and other improvements that help it generalize better to new, unseen data and avoid overfitting to training examples.\\n\\nOverall, LLaMA 2 is a more advanced and capable language model compared to the original LLaMA model, and has been shown to perform well on a wide range of natural language processing tasks.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({'query': 'What is LLaMA 2?'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

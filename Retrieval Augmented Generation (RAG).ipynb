{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34721282-c19f-4286-8593-3a737bf1b1a5",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1daf270b-8b46-4b76-9502-ab48c4259248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b001da28-7910-4c81-8ba2-6601dd2effc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOllama(model=\"llama2:13b\", \n",
    "                        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L12-v2'\n",
    ")\n",
    "store = FAISS.load_local('core_knowledge',\n",
    "                         embeddings=embedder\n",
    "                        )\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    chat_model,\n",
    "    retriever=store.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad8b353-e579-4a88-9a48-82bb80b0972a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLaMA was released on February 23, 2023."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'When was LLaMA released?',\n",
       " 'result': ' LLaMA was released on February 23, 2023.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({'query': 'When was LLaMA released?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f68f264-1a3c-46fa-8d5c-a6db18e8cd6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLaMA (LLaMA: Open and Efficient Foundation Language Models, Anthropic Harmless, and Multitask Helpful) is a series of language models developed by researchers at Meta AI. The models are designed to be open and efficient, with a focus on safety and multitask learning.\n",
      "\n",
      "LLaMA is based on the transformer architecture and was trained on a large corpus of text data. The model is available in various sizes, ranging from 1.3 billion parameters (LLaMA-B) to 6.5 billion parameters (LLaMA-7B). The larger models have been shown to achieve better performance on a variety of natural language processing tasks, such as text classification, sentiment analysis, and question answering.\n",
      "\n",
      "One of the key features of LLaMA is its ability to be finetuned for specific tasks with minimal additional data. This makes it a useful tool for a wide range of applications, from chatbots and virtual assistants to content generation and language translation. Additionally, the model is designed to be \"anthropic\" and \"harmless,\" meaning that it is trained to avoid generating content that is harmful or biased.\n",
      "\n",
      "LLaMA is an open-source project, and the model and training code are available on GitHub. This has led to a community-driven development process, with researchers and developers continuing to improve and customize the model for their own uses. Overall, LLaMA represents an important step forward in the field of language models, offering a powerful and flexible tool for natural language processing tasks."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is LLaMA?',\n",
       " 'result': ' LLaMA (LLaMA: Open and Efficient Foundation Language Models, Anthropic Harmless, and Multitask Helpful) is a series of language models developed by researchers at Meta AI. The models are designed to be open and efficient, with a focus on safety and multitask learning.\\n\\nLLaMA is based on the transformer architecture and was trained on a large corpus of text data. The model is available in various sizes, ranging from 1.3 billion parameters (LLaMA-B) to 6.5 billion parameters (LLaMA-7B). The larger models have been shown to achieve better performance on a variety of natural language processing tasks, such as text classification, sentiment analysis, and question answering.\\n\\nOne of the key features of LLaMA is its ability to be finetuned for specific tasks with minimal additional data. This makes it a useful tool for a wide range of applications, from chatbots and virtual assistants to content generation and language translation. Additionally, the model is designed to be \"anthropic\" and \"harmless,\" meaning that it is trained to avoid generating content that is harmful or biased.\\n\\nLLaMA is an open-source project, and the model and training code are available on GitHub. This has led to a community-driven development process, with researchers and developers continuing to improve and customize the model for their own uses. Overall, LLaMA represents an important step forward in the field of language models, offering a powerful and flexible tool for natural language processing tasks.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({'query': 'What is LLaMA?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b80b7c-f6df-46a1-84cb-3c70db6b50c3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

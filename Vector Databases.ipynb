{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d9814cf-9a41-43e4-80d1-ebc0a3b84a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83392cc0-b72f-450c-8583-8b4f8a1e2c54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/corey/Projects/querysaurus/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name='sentence-transformers/all-MiniLM-L12-v2'\n",
    ")\n",
    "try:\n",
    "    store = FAISS.load_local('core_knowledge',\n",
    "                             embeddings=embedder\n",
    "                            )\n",
    "    raise NotImplementedError('The code below will double-load the data if the vector db is already populated.')\n",
    "except RuntimeError:\n",
    "    store = FAISS.from_texts(texts=[''], embedding=embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f5804c4-2b15-4483-899c-57c153e66e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/document_loaders/wikipedia\n",
    "from langchain.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ed51b04-b0f5-47b9-9c74-aa0a4d3f2f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_docs = WikipediaLoader(query=\"LangChain\", load_max_docs=1, doc_content_chars_max=50000).load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db81c230-b755-4d03-991f-d39c2a101786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(langchain_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f54fb5d-07b5-43cd-a452-a38816283b39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\\n== Background ==\\nLangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. The project quickly garnered popularity, with improvements from hundreds of contributors on GitHub, trending discussions on Twitter, lively activity on the project\\'s Discord server, many YouTube tutorials, and meetups in San Francisco and London. In April 2023, LangChain had incorporated and the new startup raised over $20 million in funding at a valuation of at least $200 million from venture firm Sequoia Capital, a week after announcing a $10 million seed investment from Benchmark.\\n\\n\\n== Integrations ==\\nAs of March 2023, LangChain included integrations with systems including Amazon, Google, and Microsoft Azure cloud storage; API wrappers for news, movie information, and weather; Bash for summarization, syntax and semantics checking, and execution of shell scripts; multiple web scraping subsystems and templates; few-shot learning prompt generation support; finding and summarizing \"todo\" tasks in code; Google Drive documents, spreadsheets, and presentations summarization, extraction, and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic, and Hugging Face language models; iFixit repair guides and wikis search and summarization; MapReduce for question answering, combining documents, and question generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF file text extraction and manipulation; Python and JavaScript code generation, analysis, and debugging; Weaviate vector database to cache embedding and data objects; Redis cache database storage; Python RequestsWrapper and other methods for API requests; SQL and NoSQL databases including JSON support; Streamlit, including for logging; text mapping for k-nearest neighbors search; time zone conversion and calendar operations; tracing and recording stack symbols in threaded and asynchronous subprocess runs; and the Wolfram Alpha website and SDK. As of April 2023, it can read from more than 50 document types and data sources.\\n\\n\\n== Further reading ==\\nBriggs, James; Ingham, Francisco (2023). \"LangChain: Introduction and Getting Started\". LangChain AI Handbook. Pinecone. Retrieved 2023-04-18.\\nBuniatyan, Davit (2023). \"Ultimate Guide to LangChain & Deep Lake: Build ChatGPT to Answer Questions on Your Financial Data\". Activeloop.\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nOfficial website\\nDiscord server support hub'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b014da53-18b2-447a-9bc7-c5bc0f1216a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_to_add = []\n",
    "ids_to_add = []\n",
    "metadata_to_add = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb5867e3-7093-445b-b1b8-5954f63ac721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LangChain',\n",
       " 'summary': \"LangChain is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\",\n",
       " 'source': 'https://en.wikipedia.org/wiki/LangChain'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a19844c-0acf-4e93-9bfc-8af86269922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_pages_to_add = [\n",
    "    # Concepts\n",
    "    'Chatbot',\n",
    "    'Fine-tuning (deep learning)',\n",
    "    'Generative artificial intelligence',\n",
    "    'History of artificial intelligence',\n",
    "    'LangChain',\n",
    "    'Large language model',\n",
    "    'Language model',\n",
    "    'Prompt engineering',\n",
    "\n",
    "    # LLMs\n",
    "    'GPT-3',\n",
    "    'GPT-4',\n",
    "    'LLaMA',\n",
    "    'PaLM'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78757886-c902-4bcf-b11c-85b88a54d026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot already in vector store - skipping\n",
      "Generative artificial intelligence already in vector store - skipping\n",
      "History of artificial intelligence already in vector store - skipping\n",
      "Large language model already in vector store - skipping\n",
      "Language model already in vector store - skipping\n",
      "Prompt engineering already in vector store - skipping\n",
      "GPT-3 already in vector store - skipping\n",
      "GPT-4 already in vector store - skipping\n"
     ]
    }
   ],
   "source": [
    "for page_name in other_pages_to_add:\n",
    "    # Make sure we don't load something already loaded\n",
    "    if store.similarity_search('',\n",
    "                               filter={'title': page_name}):\n",
    "        print(f'{page_name} already in vector store - skipping')\n",
    "        continue\n",
    "    \n",
    "    for i, page_doc in enumerate(WikipediaLoader(query=page_name, load_max_docs=1, doc_content_chars_max=50000).load_and_split()):\n",
    "        docs_to_add.append(page_doc.page_content)\n",
    "        ids_to_add.append(f\"wikipedia-{page_doc.metadata['title']}-{i}\")\n",
    "        metadata_to_add.append(page_doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0095f97-e5f0-4b03-be94-c0255278f593",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wikipedia-LangChain',\n",
       " 'wikipedia-Chatbot-0',\n",
       " 'wikipedia-Chatbot-1',\n",
       " 'wikipedia-Chatbot-2',\n",
       " 'wikipedia-Chatbot-3',\n",
       " 'wikipedia-Chatbot-4',\n",
       " 'wikipedia-Chatbot-5',\n",
       " 'wikipedia-Chatbot-6',\n",
       " 'wikipedia-Chatbot-7',\n",
       " 'wikipedia-Fine-tuning (deep learning)-0',\n",
       " 'wikipedia-Generative artificial intelligence-0',\n",
       " 'wikipedia-Generative artificial intelligence-1',\n",
       " 'wikipedia-Generative artificial intelligence-2',\n",
       " 'wikipedia-Generative artificial intelligence-3',\n",
       " 'wikipedia-History of artificial intelligence-0',\n",
       " 'wikipedia-History of artificial intelligence-1',\n",
       " 'wikipedia-History of artificial intelligence-2',\n",
       " 'wikipedia-History of artificial intelligence-3',\n",
       " 'wikipedia-History of artificial intelligence-4',\n",
       " 'wikipedia-History of artificial intelligence-5',\n",
       " 'wikipedia-History of artificial intelligence-6',\n",
       " 'wikipedia-History of artificial intelligence-7',\n",
       " 'wikipedia-History of artificial intelligence-8',\n",
       " 'wikipedia-History of artificial intelligence-9',\n",
       " 'wikipedia-History of artificial intelligence-10',\n",
       " 'wikipedia-History of artificial intelligence-11',\n",
       " 'wikipedia-History of artificial intelligence-12',\n",
       " 'wikipedia-History of artificial intelligence-13',\n",
       " 'wikipedia-History of artificial intelligence-14',\n",
       " 'wikipedia-History of artificial intelligence-15',\n",
       " 'wikipedia-Large language model-0',\n",
       " 'wikipedia-Large language model-1',\n",
       " 'wikipedia-Large language model-2',\n",
       " 'wikipedia-Large language model-3',\n",
       " 'wikipedia-Large language model-4',\n",
       " 'wikipedia-Large language model-5',\n",
       " 'wikipedia-Large language model-6',\n",
       " 'wikipedia-Large language model-7',\n",
       " 'wikipedia-Large language model-8',\n",
       " 'wikipedia-Language model-0',\n",
       " 'wikipedia-Language model-1',\n",
       " 'wikipedia-Prompt engineering-0',\n",
       " 'wikipedia-Prompt engineering-1',\n",
       " 'wikipedia-Prompt engineering-2',\n",
       " 'wikipedia-Prompt engineering-3',\n",
       " 'wikipedia-Prompt engineering-4',\n",
       " 'wikipedia-Prompt engineering-5',\n",
       " 'wikipedia-Prompt engineering-6',\n",
       " 'wikipedia-GPT-3-0',\n",
       " 'wikipedia-GPT-3-1',\n",
       " 'wikipedia-GPT-3-2',\n",
       " 'wikipedia-GPT-3-3',\n",
       " 'wikipedia-GPT-3-4',\n",
       " 'wikipedia-GPT-3-5',\n",
       " 'wikipedia-GPT-4-0',\n",
       " 'wikipedia-GPT-4-1',\n",
       " 'wikipedia-GPT-4-2',\n",
       " 'wikipedia-GPT-4-3',\n",
       " 'wikipedia-GPT-4-4',\n",
       " 'wikipedia-LLaMA-0',\n",
       " 'wikipedia-LLaMA-1',\n",
       " 'wikipedia-LLaMA-2',\n",
       " 'wikipedia-PaLM-0']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e21fb07a-4d78-4eae-9415-62b7ff81bfbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wikipedia-LangChain',\n",
       " 'wikipedia-Chatbot-0',\n",
       " 'wikipedia-Chatbot-1',\n",
       " 'wikipedia-Chatbot-2',\n",
       " 'wikipedia-Chatbot-3',\n",
       " 'wikipedia-Chatbot-4',\n",
       " 'wikipedia-Chatbot-5',\n",
       " 'wikipedia-Chatbot-6',\n",
       " 'wikipedia-Chatbot-7',\n",
       " 'wikipedia-Fine-tuning (deep learning)-0',\n",
       " 'wikipedia-Generative artificial intelligence-0',\n",
       " 'wikipedia-Generative artificial intelligence-1',\n",
       " 'wikipedia-Generative artificial intelligence-2',\n",
       " 'wikipedia-Generative artificial intelligence-3',\n",
       " 'wikipedia-History of artificial intelligence-0',\n",
       " 'wikipedia-History of artificial intelligence-1',\n",
       " 'wikipedia-History of artificial intelligence-2',\n",
       " 'wikipedia-History of artificial intelligence-3',\n",
       " 'wikipedia-History of artificial intelligence-4',\n",
       " 'wikipedia-History of artificial intelligence-5',\n",
       " 'wikipedia-History of artificial intelligence-6',\n",
       " 'wikipedia-History of artificial intelligence-7',\n",
       " 'wikipedia-History of artificial intelligence-8',\n",
       " 'wikipedia-History of artificial intelligence-9',\n",
       " 'wikipedia-History of artificial intelligence-10',\n",
       " 'wikipedia-History of artificial intelligence-11',\n",
       " 'wikipedia-History of artificial intelligence-12',\n",
       " 'wikipedia-History of artificial intelligence-13',\n",
       " 'wikipedia-History of artificial intelligence-14',\n",
       " 'wikipedia-History of artificial intelligence-15',\n",
       " 'wikipedia-Large language model-0',\n",
       " 'wikipedia-Large language model-1',\n",
       " 'wikipedia-Large language model-2',\n",
       " 'wikipedia-Large language model-3',\n",
       " 'wikipedia-Large language model-4',\n",
       " 'wikipedia-Large language model-5',\n",
       " 'wikipedia-Large language model-6',\n",
       " 'wikipedia-Large language model-7',\n",
       " 'wikipedia-Large language model-8',\n",
       " 'wikipedia-Language model-0',\n",
       " 'wikipedia-Language model-1',\n",
       " 'wikipedia-Prompt engineering-0',\n",
       " 'wikipedia-Prompt engineering-1',\n",
       " 'wikipedia-Prompt engineering-2',\n",
       " 'wikipedia-Prompt engineering-3',\n",
       " 'wikipedia-Prompt engineering-4',\n",
       " 'wikipedia-Prompt engineering-5',\n",
       " 'wikipedia-Prompt engineering-6',\n",
       " 'wikipedia-GPT-3-0',\n",
       " 'wikipedia-GPT-3-1',\n",
       " 'wikipedia-GPT-3-2',\n",
       " 'wikipedia-GPT-3-3',\n",
       " 'wikipedia-GPT-3-4',\n",
       " 'wikipedia-GPT-3-5',\n",
       " 'wikipedia-GPT-4-0',\n",
       " 'wikipedia-GPT-4-1',\n",
       " 'wikipedia-GPT-4-2',\n",
       " 'wikipedia-GPT-4-3',\n",
       " 'wikipedia-GPT-4-4',\n",
       " 'wikipedia-LLaMA-0',\n",
       " 'wikipedia-LLaMA-1',\n",
       " 'wikipedia-LLaMA-2',\n",
       " 'wikipedia-PaLM-0']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.add_texts(\n",
    "    texts=docs_to_add,\n",
    "    metadatas=metadata_to_add,\n",
    "    ids=ids_to_add\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e09589c-6bc0-4ed0-8550-5fb991196ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.\\nThe field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.Eventually, it became obvious that commercial developers and researchers had grossly underestimated the difficulty of the project. In 1974, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an \"AI winter\". Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 1980s the investors became disillusioned and withdrew funding again.\\nInvestment and interest in AI boomed in the first decades of the 21st century when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets.\\n\\n\\n== Precursors ==\\n\\n\\n=== Mythical, fictional, and speculative precursors ===\\n\\n\\n==== Myth and legend ====\\nIn Greek mythology, Talos was a giant constructed of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders and would complete 3 circuits around the island\\'s perimeter daily. According to pseudo-Apollodorus\\' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos. In the Argonautica, Jason and the Argonauts defeated him by way of a single plug near his foot which, once removed, allowed the vital ichor to flow out from his body and left him inanimate.Pygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid\\'s Metamorphoses. In the 10th book of Ovid\\'s narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved.', metadata={'title': 'History of artificial intelligence', 'summary': 'The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.\\nThe field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.Eventually, it became obvious that commercial developers and researchers had grossly underestimated the difficulty of the project. In 1974, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an \"AI winter\". Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 1980s the investors became disillusioned and withdrew funding again.\\nInvestment and interest in AI boomed in the first decades of the 21st century when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets.', 'source': 'https://en.wikipedia.org/wiki/History_of_artificial_intelligence'}),\n",
       "  0.52585155),\n",
       " (Document(page_content='== AI (1993–2011) ==\\nThe field of AI, now more than a half a century old, finally achieved some of its oldest goals. It began to be used successfully throughout the technology industry, although somewhat behind the scenes. Some of the success was due to increasing computer power and some was achieved by focusing on specific isolated problems and pursuing them with the highest standards of scientific accountability. Still, the reputation of AI, in the business world at least, was less than pristine. Inside the field there was little agreement on the reasons for AI\\'s failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s. Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of \"artificial intelligence\". AI was both more cautious and more successful than it had ever been.\\n\\n\\n=== Milestones and Moore\\'s law ===\\nOn 11 May 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov. The super computer was a specialized version of a framework produced by IBM, and was capable of processing twice as many moves per second as it had during the first match (which Deep Blue had lost), reportedly 200,000,000 moves per second. The event was broadcast live over the internet and received over 74 million hits.In 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles along an unrehearsed desert trail. Two years later, a team from CMU won the DARPA Urban Challenge by autonomously navigating 55 miles in an Urban environment while adhering to traffic hazards and all traffic laws. In February 2011, in a Jeopardy! quiz show exhibition match, IBM\\'s question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.These successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computer by the 90s. In fact, Deep Blue\\'s computer was 10 million times faster than the Ferranti Mark 1 that Christopher Strachey taught to play chess in 1951. This dramatic increase is measured by Moore\\'s law, which predicts that the speed and memory capacity of computers doubles every two years, as a result of metal–oxide–semiconductor (MOS) transistor counts doubling every two years. The fundamental problem of \"raw computer power\" was slowly being overcome.', metadata={'title': 'History of artificial intelligence', 'summary': 'The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.\\nThe field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.Eventually, it became obvious that commercial developers and researchers had grossly underestimated the difficulty of the project. In 1974, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an \"AI winter\". Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 1980s the investors became disillusioned and withdrew funding again.\\nInvestment and interest in AI boomed in the first decades of the 21st century when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets.', 'source': 'https://en.wikipedia.org/wiki/History_of_artificial_intelligence'}),\n",
       "  0.6415535),\n",
       " (Document(page_content='=== AI behind the scenes ===\\nAlgorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems\\nand their solutions proved to be useful throughout the technology industry, such as\\ndata mining,\\nindustrial robotics,\\nlogistics,speech recognition,\\nbanking software,\\nmedical diagnosis\\nand Google\\'s search engine.The field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI\\'s greatest innovations have been reduced to the status of just another item in the tool chest of computer science. Nick Bostrom explains \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"Many researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, cognitive systems or computational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding. In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: \"Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.\"\\n\\n\\n== Deep learning, big data and artificial general intelligence: 2011–present ==\\nIn the first decades of the 21st century, access to large amounts of data (known as \"big data\"), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. In fact, McKinsey Global Institute estimated in their famous paper \"Big data: The next frontier for innovation, competition, and productivity\" that \"by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data\".\\nBy 2016, the market for AI-related products, hardware, and software reached more than 8 billion dollars, and the New York Times report', metadata={'title': 'History of artificial intelligence', 'summary': 'The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.\\nThe field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.Eventually, it became obvious that commercial developers and researchers had grossly underestimated the difficulty of the project. In 1974, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an \"AI winter\". Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 1980s the investors became disillusioned and withdrew funding again.\\nInvestment and interest in AI boomed in the first decades of the 21st century when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets.', 'source': 'https://en.wikipedia.org/wiki/History_of_artificial_intelligence'}),\n",
       "  0.7191825),\n",
       " (Document(page_content='=== Problems ===\\nIn the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve; all the programs were, in some sense, \"toys\". AI researchers had begun to run into several fundamental limits that could not be overcome in the 1970s. Although some of these limits would be conquered in later decades, others still stymie the field to this day.\\nLimited computer power: There was not enough memory or processing speed to accomplish anything truly useful. For example, Ross Quillian\\'s successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in memory. Hans Moravec argued in 1976 that computers were still millions of times too weak to exhibit intelligence. He suggested an analogy: artificial intelligence requires computer power in the same way that aircraft require horsepower. Below a certain threshold, it\\'s impossible, but, as power increases, eventually it could become easy. With regard to computer vision, Moravec estimated that simply matching the edge and motion detection capabilities of human retina in real time would require a general-purpose computer capable of 109 operations/second (1000 MIPS). As of 2011, practical computer vision applications require 10,000 to 1,000,000 MIPS. By comparison, the fastest supercomputer in 1976, Cray-1 (retailing at $5 million to $8 million), was only capable of around 80 to 130 MIPS, and a typical desktop computer at the time achieved less than 1 MIPS.\\nIntractability and the combinatorial explosion. In 1972 Richard Karp (building on Stephen Cook\\'s 1971 theorem) showed there are many problems that can probably only be solved in exponential time (in the size of the inputs). Finding optimal solutions to these problems requires unimaginable amounts of computer time except when the problems are trivial. This almost certainly meant that many of the \"toy\" solutions used by AI would probably never scale up into useful systems.\\nCommonsense knowledge and reasoning. Many important artificial intelligence applications like vision or natural language require simply enormous amounts of information about the world: the program needs to have some idea of what it might be looking at or what it is talking about. This requires that the program know most of the same things about the world that a child does. Researchers soon discovered that this was a truly vast amount of information. No one in 1970 could build a database so large and no one knew how a program might learn so much information.\\nMoravec\\'s paradox: Proving theorems and solving geometry problems is comparatively easy for computers, but a supposedly simple task like recognizing a face or crossing a room without bumping into anything is extremely difficult. This helps explain why research into vision and robotics had made so little progress by the middle 1970s.\\nThe frame and qualification problems. AI researchers (like John McCarthy) who used logic discovered that they could not represent ordinary deductions that involved planning or default reasoning without making changes to the structure of logic itself. They developed new logics (like non-monotonic logics and modal logics) to try to solve the problems.\\n\\n\\n=== End of funding ===', metadata={'title': 'History of artificial intelligence', 'summary': 'The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.\\nThe field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.Eventually, it became obvious that commercial developers and researchers had grossly underestimated the difficulty of the project. In 1974, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an \"AI winter\". Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 1980s the investors became disillusioned and withdrew funding again.\\nInvestment and interest in AI boomed in the first decades of the 21st century when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets.', 'source': 'https://en.wikipedia.org/wiki/History_of_artificial_intelligence'}),\n",
       "  0.8176464)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.similarity_search_with_score('Who invented AI?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1690904-b5d2-41f8-a3b5-e02439fcbc9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='== Modalities ==\\nA generative AI system is constructed by applying unsupervised or self-supervised machine learning to a data set. The capabilities of a generative AI system depend on the modality or type of the data set used.\\nGenerative AI can be either unimodal or multimodal; unimodal systems take only one type of input, whereas multimodal systems can take more than one type of input. For example, one version of OpenAI\\'s GPT-4 accepts both text and image inputs.\\n\\n\\n=== Text ===\\n\\nGenerative AI systems trained on words or word tokens include GPT-3, LaMDA, LLaMA, BLOOM, GPT-4, and others (see List of large language models). They are capable of natural language processing, machine translation, and natural language generation and can be used as foundation models for other tasks. Data sets include BookCorpus, Wikipedia, and others (see List of text corpora).\\n\\n\\n=== Code ===\\nIn addition to natural language text, large language models can be trained on programming language text, allowing them to generate source code for new computer programs. Examples include OpenAI Codex.\\n\\n\\n=== Images ===\\nGenerative AI systems trained on sets of images with text captions include Imagen, DALL-E, Midjourney, Adobe Firefly, Stable Diffusion and others (see Artificial intelligence art, Generative art, and Synthetic media). They are commonly used for text-to-image generation and neural style transfer. Datasets include LAION-5B and others (See Datasets in computer vision).\\n\\n\\n=== Music ===\\nGenerative AI systems such as MusicLM and MusicGen can be trained on the audio waveforms of recorded music along with text annotations, in order to generate new musical samples based on text descriptions such as a calming violin melody backed by a distorted guitar riff.\\n\\n\\n=== Video ===\\nGenerative AI trained on annotated video can generate temporally-coherent video clips. Examples include Gen1 and Gen2 by RunwayML and Make-A-Video by Meta Platforms.\\n\\n\\n=== Molecules ===\\nGenerative AI systems can be trained on sequences of amino acids or molecular representations such as SMILES representing DNA or proteins. These systems, such as AlphaFold, are used for protein structure prediction and drug discovery. Datasets include various biological datasets.\\n\\n\\n=== Robotics ===\\nGenerative AI trained on the motions of a robotic system can generate new trajectories for motion planning or navigation. For example, UniPi from Google Research uses prompts like \"pick up blue bowl\" or \"wipe plate with yellow sponge\" to control movements of a robot arm. Multimodal \"vision-language-action\" models such as Google\\'s RT-2 can perform rudimentary reasoning in response to user prompts and visual input, such as picking up a toy dinosaur when given the prompt pick up the extinct animal at a table filled with toy animals and other objects.\\n\\n\\n== Awards and recognition ==\\nIn both 1991 and 1992, Karl Sims won the Golden Nica award at Prix Ars Electronica for his 3D AI animated videos using artificial evolution.In 2009, Eric Millikin won the Pulitzer Prize along with several other awards for his artificial intelligence art that was critical of government corruption in Detroit and resulted in the city\\'s mayor being sent to jail.In 2018 Christie\\'s auction house in New York sold an artificial intelligence work, \"Edmond de Bellamy\" for US$432,500. It was created by a collective in Paris named \"Obvious\". In 2019, Stephanie Dinkins won the Creative Capital award for her creation of an evolving artificial intelligence based on the \"interests and culture(s) of people of color.\" Also in 2019, Sougwen Chung won the Lumen Prize for her performances with a robotic arm that uses AI to attempt to draw in a manner similar to Chung.In 2022, an amateur artist using Midjourney won the first-place $300 prize in a digital art competition at the Colorado State Fair. Also in 2022, Refik Anadol created an artificial intelligence art installation at the Museum of Modern Art in New York, based on the museum\\'s own collection.', metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (AI) is artificial intelligence capable of generating text, images, or other media, using generative models. Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics.In the early 2020s, advances in transformer-based deep neural networks enabled a number of generative AI systems notable for accepting natural language prompts as input. These include large language model chatbots such as ChatGPT, Bing Chat, Bard, and LLaMA, and text-to-image artificial intelligence art systems such as Stable Diffusion, Midjourney, and DALL-E.Generative AI has uses across a wide range of industries, including art, writing, software development, product design, healthcare, finance, gaming, marketing, and fashion.\\nInvestment in generative AI surged during the early 2020s, with large companies such as Microsoft, Google, and Baidu as well as numerous smaller firms developing generative AI models. However, there are also concerns about the potential misuse of generative AI, including cybercrime or creating fake news or deepfakes which can be used to deceive or manipulate people.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}),\n",
       "  0.74503464),\n",
       " (Document(page_content='== Software and Hardware ==\\nGenerative AI models are used to power chatbot products such as ChatGPT, programming tools such as GitHub Copilot, text-to-image products such as Midjourney, and text-to-video products such as Runway Gen-2. Generative AI features have been integrated into a variety of existing commercially-available products such as Microsoft Office, Google Photos, and Adobe Photoshop.  Many generative AI models are also available as open-source software, including Stable Diffusion and the LLaMA language model.\\nSmaller generative AI models with up to a few billion parameters can run on smartphones, embedded devices, and personal computers. For example, LLaMA-7B (a version with 7 billion parameters) can run on a Raspberry Pi 4 and one version of Stable Diffusion can run on an iPhone 11.Larger models with tens of billions of parameters can run on laptop or desktop computers. To achieve an acceptable speed, models of this size may require accelerators such as the GPU chips produced by Nvidia and AMD or the Neural Engine included in Apple silicon products. For example, the 65 billion parameter version of LLaMA can be configured to run on a desktop PC.Language models with hundreds of billions of parameters, such as GPT-4 or PaLM, typically run on datacenter computers equipped with arrays of GPUs (such as Nvidia\\'s H100) or AI accelerator chips (such as Google\\'s TPU). These very large models are typically accessed as cloud services over the Internet.\\nIn 2022, the United States New Export Controls on Advanced Computing and Semiconductors to China imposed restrictions on exports to China of GPU and AI accelerator chips used for generative AI. Chips such as the Nvidia A800 and the Biren Technology BR104 were developed to meet the requirements of the sanctions.\\n\\n\\n== Concerns ==\\n\\nThe development of generative AI has raised concerns from governments, businesses, and individuals, resulting in protests, legal actions, calls to pause AI experiments, and actions by multiple governments. In a July 2023 briefing of the United Nations Security Council, Secretary-General António Guterres stated \"Generative AI has enormous potential for good and evil at scale\", that AI may \"turbocharge global development\" and contribute between $10 and $15 trillion to the global economy by 2030, but that its malicious use \"could cause horrific levels of death and destruction, widespread trauma, and deep psychological damage on an unimaginable scale\".\\n\\n\\n=== Job losses ===\\n\\nFrom the early days of the development of AI there have been arguments put forward by ELIZA creator Joseph Weizenbaum and others about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculations and qualitative, value-based judgements. In April 2023, it was reported that image generation AI has resulted in 70% of the jobs for video game illustrators in China being lost. In July 2023, developments in generative AI contributed to the 2023 Hollywood labor disputes. Fran Drescher, president of the Screen Actors Guild, declared that \"artificial intelligence poses an existential threat to creative professions\" during the 2023 SAG-AFTRA strike.\\n\\n\\n=== Deepfakes ===\\nDeepfakes (a portmanteau of \"deep learning\" and \"fake\") are AI-generated media that take a person in an existing image or video and replace them with someone else\\'s likeness using artificial neural networks. Deepfakes have garnered widespread attention and concerns for their uses in deepfake celebrity pornographic videos, revenge porn, fake news, hoaxes, and financial fraud. This has elicited responses from both industry and government to detect and limit their use.', metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (AI) is artificial intelligence capable of generating text, images, or other media, using generative models. Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics.In the early 2020s, advances in transformer-based deep neural networks enabled a number of generative AI systems notable for accepting natural language prompts as input. These include large language model chatbots such as ChatGPT, Bing Chat, Bard, and LLaMA, and text-to-image artificial intelligence art systems such as Stable Diffusion, Midjourney, and DALL-E.Generative AI has uses across a wide range of industries, including art, writing, software development, product design, healthcare, finance, gaming, marketing, and fashion.\\nInvestment in generative AI surged during the early 2020s, with large companies such as Microsoft, Google, and Baidu as well as numerous smaller firms developing generative AI models. However, there are also concerns about the potential misuse of generative AI, including cybercrime or creating fake news or deepfakes which can be used to deceive or manipulate people.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}),\n",
       "  0.7764249),\n",
       " (Document(page_content='Generative artificial intelligence (AI) is artificial intelligence capable of generating text, images, or other media, using generative models. Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics.In the early 2020s, advances in transformer-based deep neural networks enabled a number of generative AI systems notable for accepting natural language prompts as input. These include large language model chatbots such as ChatGPT, Bing Chat, Bard, and LLaMA, and text-to-image artificial intelligence art systems such as Stable Diffusion, Midjourney, and DALL-E.Generative AI has uses across a wide range of industries, including art, writing, software development, product design, healthcare, finance, gaming, marketing, and fashion.\\nInvestment in generative AI surged during the early 2020s, with large companies such as Microsoft, Google, and Baidu as well as numerous smaller firms developing generative AI models. However, there are also concerns about the potential misuse of generative AI, including cybercrime or creating fake news or deepfakes which can be used to deceive or manipulate people.\\n\\n\\n== History ==\\n\\nThe academic discipline of artificial intelligence was founded at a research workshop at Dartmouth College in 1956, and has experienced several waves of advancement and optimism in the decades since. Since its founding, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity. These concepts of automated art date back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music.  The tradition of creative automatons has flourished throughout history, such as Maillardet\\'s automaton, created in the early 1800s.Since the founding of AI in the 1950s, artists and researchers have used generative artificial intelligence to create new works. By the early 1970s, Harold Cohen was creating and exhibiting works created by AARON, the computer program Cohen created to generate paintings.The field of machine learning often uses statistical models, including generative models, to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models, due to the difficulty of generative modeling.In 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative, rather than discriminative, models of complex data such as images. These deep generative models were the first able to output not only class labels for images, but to output entire images.\\nIn 2017, the Transformer network enabled advancements in generative models, leading to the first generative pre-trained transformer (GPT) in 2018. This was followed in 2019 by GPT-2 which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model.In 2021, the release of DALL-E, a transformer-based pixel generative model, followed by Midjourney and Stable Diffusion marked the emergence of practical high-quality artificial intelligence art from natural language prompts.\\nIn March 2023, GPT-4 was released. A team from Microsoft Research argued that \"it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system\".', metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (AI) is artificial intelligence capable of generating text, images, or other media, using generative models. Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics.In the early 2020s, advances in transformer-based deep neural networks enabled a number of generative AI systems notable for accepting natural language prompts as input. These include large language model chatbots such as ChatGPT, Bing Chat, Bard, and LLaMA, and text-to-image artificial intelligence art systems such as Stable Diffusion, Midjourney, and DALL-E.Generative AI has uses across a wide range of industries, including art, writing, software development, product design, healthcare, finance, gaming, marketing, and fashion.\\nInvestment in generative AI surged during the early 2020s, with large companies such as Microsoft, Google, and Baidu as well as numerous smaller firms developing generative AI models. However, there are also concerns about the potential misuse of generative AI, including cybercrime or creating fake news or deepfakes which can be used to deceive or manipulate people.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}),\n",
       "  0.8399817),\n",
       " (Document(page_content='=== AI behind the scenes ===\\nAlgorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems\\nand their solutions proved to be useful throughout the technology industry, such as\\ndata mining,\\nindustrial robotics,\\nlogistics,speech recognition,\\nbanking software,\\nmedical diagnosis\\nand Google\\'s search engine.The field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI\\'s greatest innovations have been reduced to the status of just another item in the tool chest of computer science. Nick Bostrom explains \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"Many researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, cognitive systems or computational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding. In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: \"Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.\"\\n\\n\\n== Deep learning, big data and artificial general intelligence: 2011–present ==\\nIn the first decades of the 21st century, access to large amounts of data (known as \"big data\"), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. In fact, McKinsey Global Institute estimated in their famous paper \"Big data: The next frontier for innovation, competition, and productivity\" that \"by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data\".\\nBy 2016, the market for AI-related products, hardware, and software reached more than 8 billion dollars, and the New York Times report', metadata={'title': 'History of artificial intelligence', 'summary': 'The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.\\nThe field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.Eventually, it became obvious that commercial developers and researchers had grossly underestimated the difficulty of the project. In 1974, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an \"AI winter\". Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 1980s the investors became disillusioned and withdrew funding again.\\nInvestment and interest in AI boomed in the first decades of the 21st century when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets.', 'source': 'https://en.wikipedia.org/wiki/History_of_artificial_intelligence'}),\n",
       "  1.0452995)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.similarity_search_with_score('What are the benefits of generative AI?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2885554a-20a7-4fae-bf20-bab1bf8ee490",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.save_local('core_knowledge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4de212d-3def-48c6-8028-cd83c5d1bc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Corpus of Linguistic Acceptability\\nGLUE benchmark\\nMicrosoft Research Paraphrase Corpus\\nMulti-Genre Natural Language Inference\\nQuestion Natural Language Inference\\nQuora Question Pairs\\nRecognizing Textual Entailment\\nSemantic Textual Similarity Benchmark\\nSQuAD question answering Test\\nStanford Sentiment Treebank\\nWinograd NLI\\nBoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, OpenBookQA, NaturalQuestions, TriviaQA, RACE, MMLU (Massive Multitask Language Understanding), BIG-bench hard, GSM8k, RealToxicityPrompts, WinoGender, CrowS-Pairs. (LLaMa Benchmark)\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==', metadata={'title': 'Language model', 'summary': 'A language model is a probabilistic model of a natural language that can generate probabilities of a series of words, based on text corpora in one or multiple languages it was trained on. Large language models, as their most advanced form, are a combination of feedforward neural networks and transformers. They have superseded recurrent neural network-based models, which had previously superseded the pure statistical models, such as word n-gram language model.\\nLanguage models are useful for a variety of tasks, including speech recognition (helping prevent predictions of low-probability (e.g. nonsense) sequences), machine translation, natural language generation (generating more human-like text), optical character recognition, handwriting recognition, grammar induction, information retrieval, and other.', 'source': 'https://en.wikipedia.org/wiki/Language_model'})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.similarity_search('',\n",
    "                       filter={'title': 'Language model'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9b1d25-a302-4ba6-8efb-1143899b75fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
